<!DOCTYPE HTML>
<!--
	Landed by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Comparing Bayesian and LSTM Networks in Natural Language Generation</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>
		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">
					<h1 id="logo"><a target="_blank" href="https://github.com/apropos13"> Github</a> <a target="_blank" href="https://github.com/apropos13" class="icon alt fa-github"> </a></h1>
					<nav id="nav">
						<ul>
							<li><a href="index.html">Home</a></li>
							<li>
								<a href="#">Other Projects</a>
								<ul>
									<li><a href="thesis.html">End-to-End Natural Language Generation using LSTM-based Neural Networks</a></li>
									<li><a href="dm.html">Data Mining Projects</a></li>
									<!--<li>
										<a href="#">Submenu</a>
										<ul>
											<li><a href="#">Option 1</a></li>
											<li><a href="#">Option 2</a></li>
											<li><a href="#">Option 3</a></li>
											<li><a href="#">Option 4</a></li>
										</ul>
									</li>
								-->
								</ul>
							</li>
							<li><a href="snippets.html">Code Snippets</a></li>
							
						</ul>
					</nav>
				</header>

			<!-- Main -->
				<div id="main" class="wrapper style1">
					<div class="container">
						<header class="major">
							<h2>Comparing Bayesian and LSTM Networks in Natural Language Generation
							</h2>
							<p>A phrase based vs. a word based natural language generation model</p>
						</header>
						<div class="row 150%">
							<div class="8u 12u$(medium)">

								<!-- Content -->
									<section id="content">
										<a href="#" class="image fit"><img src="images/bayes_net.png" alt="" /></a>
										<h3>Results</h3>
										<p>In this paper, we compare two very different models for performing NLG. We first present BAGEL, which is a language generator model that utilizes dynamic Bayesian networks in order to produce natural and informative utterances. This NLG system learns content ordering, lexical selection, aggregation and realization directly from data, and it requires no handcrafting beyond the semantic stack annotation of the training data. The second NLG model that this paper presents is based on semantically conditioned LSTM structures that can learn from unaligned data. After experimentation, we observe that the former model outperforms the latter in terms of BLEU score. We attribute this performance difference to the fact that the neural model, not capable of taking advantage of semantic alignment, requires substantially more data in order to be properly trained. As we see in our experiments, the small size of the dataset does not affect the Bayesian model, which exploits the semantic annotation of the data and, hence, requires fewer data instances to learn the desired probabilities.</p>

										

										<a href="#" class="image fit"><img src="images/stack_backoff.jpg" alt="" /></a>

										<h3>Summary</h3>
										<p>First of all, we performed experiments to demonstrate the impact of the back-off scheme (refer to "Generalizing"). We can observe two essential phenomena in the figure above. With no phrase back-off (i.e. level 0), fewer utterances tend to be produced as we increase the level of stack back-off. On the other hand, when the full phrase back-off (i.e. level 6) is enabled, then we see that the surface realization becomes increasingly successful with the increasing depth of stack back-off. 
</p>
										<p>These may seem contradictory at first glance. However, the former is the result of the fact that the predicted stack sequences using back-off are more likely to have not been present in the training set. This not only is not an issue when using the phrase back-off, but it also gives the phrase inference an opportunity to explore more candidates beyond the training set from which the model learned. Let us also point out that using full back-off for both the stack and the phrase inference enables the model to produce an utterance for every single test MR, if the model is trained on 90% or 95% of the dataset (and tested on the remainder). If the proportion of the training data is lower, the success rate remains above 98% nonetheless, which is rather impressive considering that the lowest proportion, i.e. 40% of the dataset, corresponds to mere 161 samples.
										</p>
									</section>


									<section>
								<h3>Result Summary</h3>
								<h4>BLEU score evaluation</h4>
								<div class="table-wrapper">
									<table class="alt">
										<thead>
											<tr>
												<th>Training Proportion</th>
												<th>DBN Score</th>
												<th>LSTM Score</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td>40%</td>
												<td>0.639</td>
												<td>0.304 </td>
											</tr>
											<tr>
												<td>50%</td>
												<td>0.637</td>
												<td>0.271 </td>
											</tr>
											<tr>
												<td>60%</td>
												<td> 0.640 </td>
												<td>0.245 </td>
											</tr>
											<tr>
												<td>70%</td>
												<td>0.640 </td>
												<td>0.249 </td>
											</tr>
											<tr>
												<td>80%</td>
												<td>0.644 </td>
												<td>0.231 </td>
											</tr>
											<tr>
												<td>90%</td>
												<td>0.649</td>
												<td>0.136 </td>
											</tr>
											<tr>
												<td>95%</td>
												<td>0.646</td>
												<td>0.190</td>
											</tr>
										</tbody>
									</table>
								</div>
							</section>


							</div>
							<div class="4u$ 12u$(medium)">

								<!-- Sidebar -->
									<section id="sidebar">
										<section>
											<h3>Project Goal</h3>
											<p>In this paper we focus on the comparison of two very different methods to perform natural language generation. We use a graphical and a deep learning approach in order to produce utterances given meaning representations in the restaurant domain. The first part of this paper presents a fully data-driven generation method that performs the language generation task as a search over the most likely sequence of surface realization phrases according to Factored Language Models. The second part of this paper focuses on a statistical generator based on Long Short-Term Memory neural networks (LSTMs). This model learns from unaligned data and jointly performs the sentence planning and the surface realization. We report our results after experimenting on the BAGEL dataset produced by 42 untrained Amazon Mechanical Turkers.</p>
											<footer>
												<ul class="actions">
													<li><a target="_blank" href="https://github.com/apropos13/Bayesian-and-LSTM-Networks-in-NLG" class="button">Github Page</a></li>
												</ul>
											</footer>
										</section>
										<hr />
										<section>
											<a href="#" class="image fit"><img src="images/backoff.png" alt="" /></a>
											<h3>Generalizing</h3>
											<p>The model does not easily generalize to unseen semantic stacks, since if a specific stack-phrase configuration is not observed during training, then the probability of observing  phrase given a stack is 0. In order to account for this fact we introduce <i> back-off </i> schemes which get activated only if we encounter a stack configuration not seen in training. In order to implement the back-off schemes we follow the approach suggested in Mairesse et al. (2010) and make each realization phrase dependent on <i> understack </i> configurations, namely the <i>tail</i> and the <i> head </i> of the stack.</p>
										</section>
									</section>

							</div>
						</div>
					</div>
				</div>

			<!-- Footer -->
				<footer id="footer">
					<ul class="icons">
						<!-- <li><a href="#" class="icon alt fa-twitter"><span class="label">Twitter</span></a></li> -->
						<!-- <li><a href="#" class="icon alt fa-facebook"><span class="label">Facebook</span></a></li> -->
						<li><a target="_blank" href="https://www.linkedin.com/in/panos-karagiannis-047987138/" class="icon alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<!-- <li><a href="#" class="icon alt fa-instagram"><span class="label">Instagram</span></a></li> -->
						<li><a target="_blank" href="https://github.com/apropos13" class="icon alt fa-github"><span class="label">GitHub</span></a></li>
						<!--- <li><a href="#" class="icon alt fa-envelope"><span class="label">panoskaragiannis.ucla@gmail</span></a></li> -->
					</ul>
					<ul class="copyright">
						<li>&copy; Panos Karagiannis. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>