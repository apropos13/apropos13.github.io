<!DOCTYPE HTML>
<!--
	Landed by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>End-to-End Natural Language Generation using LSTM-based Neural Networks</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>
		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">
					<h1 id="logo"><a target="_blank" href="https://github.com/apropos13"> Github</a> <a target="_blank" href="https://github.com/apropos13" class="icon alt fa-github"> </a></h1>
					<nav id="nav">
						<ul>
							<li><a href="index.html">Home</a></li>
							<li>
								<a href="#">Other Projects</a>
								<ul>
									<li><a href="compare_bayes.html">Comparing Bayesian and LSTM Networks in Natural Language Generation</a></li>
									<li><a href="dm.html">Data Mining Projects</a></li>
									<!--
									<li>
										<a href="#">Submenu</a>
										<ul>
											<li><a href="#">Option 1</a></li>
											<li><a href="#">Option 2</a></li>
											<li><a href="#">Option 3</a></li>
											<li><a href="#">Option 4</a></li>
										</ul>
									-->
									</li>
								</ul>
							</li>
							<li><a href="snippets.html">Code Snippets</a></li>
							
						</ul>
					</nav>
				</header>

			<!-- Main -->
				<div id="main" class="wrapper style1">
					<div class="container">
						<header class="major">
							<h2>End-to-End Natural Language Generation using LSTM-based Neural Networks</h2>
							<p>Master Thesis</p>
						</header>
						<div class="row 150%">
							<div class="4u 12u$(medium)">

								<!-- Sidebar -->
									<section id="sidebar">
										<section>
											<h3>Project Goal</h3>
											<p>Deep learning project developed as my master thesis but also for submission to the E2E NLG Challenge. The project utilizes neural networks to learn from unaligned data in order to jointly perform sentence planning and surface realizations. It uses an Encoder â€“ Decoder architecture for sequence to sequence modeling.
											</p>
											<footer>
												<ul class="actions">
													<li><a target="_blank" href="https://github.com/apropos13/Thesis-E2E-Challenge-/tree/master/e2e-nlg-challenge" class="button">Project Github Page </a></li>
												</ul>
											</footer>
										</section>
										<hr />
										<section>
											<a href="#" class="image fit"><img src="images/mr.png" alt="" /></a>
											<h3>Inform Dialogue Act</h3>
											<p>An example of a meaning representation (MR) and a corresponding utterance.</p>
											<!-- <footer>
												<ul class="actions">
													<li><a href="#" class="button">Learn More</a></li>
												</ul>
											</footer> -->
										</section>
									</section>

							</div>
							<div class="8u$ 12u$(medium) important(medium)">

								<!-- Content -->
									<section id="content">
										<a href="#" class="image fit"><img src="images/encoder_decoder_bright.jpg" alt="" /></a>
										<h3>Introduction</h3>
										<p>Natural language generation lies at the core of generative dialogue systems and conversational agents. This paper studies two statistical generators based on Long Short-term Memory neural networks. Both networks learn from unaligned data and jointly perform sentence planning and surface realizations. </p>
										<p>This is a promising new approach for natural language generation, since it eliminates the need for hard-coded heuristic rules and hence is easily generalizable to new domains. FInally, post processing is performed on the output from the neural network architecture, to make the utterances more readable and natural.</p>

										<h3>Approach</h3>
										<p>With the continuous efforts to develop general artificial intelligence, there has recently been a substantial amount of research done in the fields of natural language processing (NLP) and generation (NLG). It gave rise to popular digital personal assistants in smartphones, such as Siri or Cortana, as well as separate devices with the sole purpose of offering the services of such personal assistants through interactive communication, such as Amazon Echo (powered by the Alexa assistant) or Google Home. The capabilities of these conversational agents are still fairly limited and lacking in various aspects, ranging from distinguishing the sentiment of human utterances to producing utterances with human-like coherence and naturalness. <p> In our work we focus on the NLG module in task-oriented dialogue systems, in particular on generation of textual utterances from structured <i> meaning representations </i> (MRs). An MR describes a single dialogue act in the form of a list of pieces of information that need to be conveyed to the other party in the dialogue (typically a human user). Each piece of information is represented by a slot-value pair, where the <i> slot </i> identifies the type of information and the <i> value </i> is the corresponding content (see image on the left). <i> Dialogue acts </i>(DAs) can be of different types, depending on the intent of the dialogue manager component. They can range from simple ones, such as a <i> goodbye </i> DA with no slots at all, to complex ones, such as an <i> inform </i> DA containing multiple slots with various types of values. </p>

										<h3>Key points:</h3>
</p>
										<ul>
											<li>We have implemented an encoder-decoder LSTM model and augmented its capabilities with a variety of different post-processing steps.</li>
											<li>The model is capable of generating reasonable utterances, but there are still margins for improvement. </li>
											<li>This is where the attention mechanism comes to the rescue (see latest project/publication).</li>
											<li> It is an interesting task to attempt translating the entire training utterance into an appropriate slot value representation. We hypothesize this technique will result in better transitions between slot values from a given meaning representation and work as an automatic method for aligning our currently unaligned data.</li>
											<li> We also plan on incorporating beam search to our decoder in order to efficiently identify the most probable output sequences. </li>
										</ul>
									</section>

							</div>
						</div>
					</div>
				</div>

			<!-- Footer -->
				<footer id="footer">
					<ul class="icons">
						<!-- <li><a href="#" class="icon alt fa-twitter"><span class="label">Twitter</span></a></li> -->
						<!-- <li><a href="#" class="icon alt fa-facebook"><span class="label">Facebook</span></a></li> -->
						<li><a target="_blank" href="https://www.linkedin.com/in/panos-karagiannis-047987138/" class="icon alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<!-- <li><a href="#" class="icon alt fa-instagram"><span class="label">Instagram</span></a></li> -->
						<li><a target="_blank" href="https://github.com/apropos13" class="icon alt fa-github"><span class="label">GitHub</span></a></li>
						<!--- <li><a href="#" class="icon alt fa-envelope"><span class="label">panoskaragiannis.ucla@gmail</span></a></li> -->
					</ul>
					<ul class="copyright">
						<li>&copy; Panos Karagiannis. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>